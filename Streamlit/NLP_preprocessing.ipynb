{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Steps for Preprocessing of Text in NLP"
      ],
      "metadata": {
        "id": "VMz816EH6O7R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Remove Newlines and Tabs\n",
        "- Remove HTML Tags\n",
        "- Remove Links\n",
        "- Remove White spaces\n",
        "- Remove Accented Words\n",
        "- Case Coversion\n",
        "- Remove Repeated Character and Punctuation\n",
        "- Expand Contraction Words\n",
        "- Remove Special Characters\n",
        "- Remove Stop Words\n",
        "- Spelling Coreections\n",
        "- Lematization (converting words to its root words)\n"
      ],
      "metadata": {
        "id": "ntUnlH2v6fqy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Packages"
      ],
      "metadata": {
        "id": "0XzoXzjf_IBd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unidecode\n",
        "!pip install autocorrect\n",
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GzA7LtZ3AHRp",
        "outputId": "28a8f234-c125-4c38-d40a-50e1ecff088c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.3.6-py3-none-any.whl (235 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.9/235.9 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.3.6\n",
            "Collecting autocorrect\n",
            "  Downloading autocorrect-2.6.1.tar.gz (622 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m622.8/622.8 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: autocorrect\n",
            "  Building wheel for autocorrect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for autocorrect: filename=autocorrect-2.6.1-py3-none-any.whl size=622363 sha256=7f9c3ce6ce2b4cd35a070dd6ed38539d4bc4c00edaa49aceb6170a00766c24f2\n",
            "  Stored in directory: /root/.cache/pip/wheels/b5/7b/6d/b76b29ce11ff8e2521c8c7dd0e5bfee4fb1789d76193124343\n",
            "Successfully built autocorrect\n",
            "Installing collected packages: autocorrect\n",
            "Successfully installed autocorrect-2.6.1\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.65.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import time\n",
        "import unidecode\n",
        "import nltk\n",
        "import zipfile\n",
        "import io\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from autocorrect import Speller\n",
        "import string\n",
        "import timeit\n",
        "from bs4 import BeautifulSoup"
      ],
      "metadata": {
        "id": "HeIPlmAg_N1V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1994960b-c7c0-43ad-bd23-2d0884abf759"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Remove Newlines and Tabs"
      ],
      "metadata": {
        "id": "m9TzjmReDZgG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def new_line_tabs(text):\n",
        "  result = text.replace('\\\\',' ').replace('\\\\n', ' ').replace('\\t', ' ').replace('\\n', ' ').replace('. com', '.com')\n",
        "  return result\n",
        "check = 'This is her \\\\ first day at this place.\\n Please,\\t Be nice to her.\\\\n'\n",
        "print(check)\n",
        "check1 = new_line_tabs(check)\n",
        "print(check1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jByafKtj8D_h",
        "outputId": "3940b682-9c15-415c-87b3-bb461465a032"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is her \\ first day at this place.\n",
            " Please,\t Be nice to her.\\n\n",
            "This is her   first day at this place.  Please,  Be nice to her. n\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Removing HTML Tags"
      ],
      "metadata": {
        "id": "MWKatmqhDjZV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_HTML_Tags(text):\n",
        "  soup = BeautifulSoup(text, 'html.parser')\n",
        "  result = soup.getText(separator = ' ')\n",
        "  return result\n",
        "check = 'This is a <b>nice</b> place to live.<h1>Hello Wolrld</h1>'\n",
        "print(check)\n",
        "check1 = remove_HTML_Tags(check)\n",
        "print(check1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "niBdch4v-B9C",
        "outputId": "0ceff392-6190-411c-e0d3-8208c4df0341"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is a <b>nice</b> place to live.<h1>Hello Wolrld</h1>\n",
            "This is a  nice  place to live. Hello Wolrld\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Remove Links"
      ],
      "metadata": {
        "id": "TO_QrVuiE4vJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_links(text):\n",
        "  rem_http = re.sub(r'http\\S+', '', text)\n",
        "  rem_com = re.sub(r'\\ [A-Za-z]*\\.com',' ',rem_http)\n",
        "  return rem_com\n",
        "check = ' website: catster.com  visit: https://catster.com//how-to-feed-cats adasfas ad as'\n",
        "print(check)\n",
        "check1 = remove_links(check)\n",
        "print(check1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VR6_p_gzEYi8",
        "outputId": "f2c9e47a-a6d1-42e2-bd47-29def0d1fb8b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " website: catster.com  visit: https://catster.com//how-to-feed-cats adasfas ad as\n",
            " website:   visit:  adasfas ad as\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Remove WhiteSpaces"
      ],
      "metadata": {
        "id": "VWu_l5VRG6-e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_whitespaces(text):\n",
        "  pattern = re.compile(r'\\s+')\n",
        "  rws = re.sub(pattern, ' ', text)\n",
        "  result = rws.replace('?', ' ? ').replace(')', ') ')\n",
        "  return result\n",
        "check = 'How   are  \\t you \\n   doing? (pakistan)ABC hello                world!'\n",
        "print(check)\n",
        "check1 = remove_whitespaces(check)\n",
        "print(check1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qm0hgGkYGNSM",
        "outputId": "5bf92b46-b039-481a-8c68-d5397d461afb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "How   are  \t you \n",
            "   doing? (pakistan)ABC hello                world!\n",
            "How are you doing ?  (pakistan) ABC hello world!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Remove Accented Words"
      ],
      "metadata": {
        "id": "0j5F098TIznS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "xOHzNzIBIzHY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_accented_Word(text):\n",
        "  result = unidecode.unidecode(text)\n",
        "  return result\n",
        "check = 'Málaga, àéêöhello'\n",
        "print(check)\n",
        "check1 = remove_accented_Word(check)\n",
        "print(check1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a-HEqxJoIHdy",
        "outputId": "3624eda7-8661-4643-82f7-e3c96bce91be"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Málaga, àéêöhello\n",
            "Malaga, aeeohello\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Case Conversion"
      ],
      "metadata": {
        "id": "Dvna-5hXJYke"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def case_conversion(text):\n",
        "  if isinstance(text, pd.Series):\n",
        "      result =  text.str.lower()\n",
        "      return result\n",
        "  elif isinstance(text, str):\n",
        "      result =  text.lower()\n",
        "      return result\n",
        "\n",
        "check = 'Pakistan Zinda BAD!'\n",
        "print(check)\n",
        "check1 = case_conversion(check)\n",
        "print(check1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DOM5QHlcJTwq",
        "outputId": "a4b19606-e104-4871-a3c4-9cf9da7fd661"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pakistan Zinda BAD!\n",
            "pakistan zinda bad!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## reduce repeated characters"
      ],
      "metadata": {
        "id": "sARWdA1CcPLe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def reduce_repeated_characters(text):\n",
        "    Pattern_alpha = re.compile(r\"([A-Za-z])\\1{1,}\", re.DOTALL)\n",
        "    Formatted_text = Pattern_alpha.sub(r\"\\1\\1\", text)\n",
        "    Pattern_Punct = re.compile(r'([.,/#!$%^&*?;:{}=_`~()+-])\\1{1,}')\n",
        "    Combined_Formatted = Pattern_Punct.sub(r'\\1', Formatted_text)\n",
        "    Final_Formatted = re.sub(' {2,}',' ', Combined_Formatted)\n",
        "    return Final_Formatted\n",
        "\n",
        "check = 'Realllllllllyyyyy,        Greeeeaaaatttt   !!!!?....;;;;:)'\n",
        "print(check)\n",
        "check1 = reduce_repeated_characters(check)\n",
        "print(check1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-F_Q_0hlcOcj",
        "outputId": "dc6e069f-3cc2-48ac-ba01-970ddb729a8b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Realllllllllyyyyy,        Greeeeaaaatttt   !!!!?....;;;;:)\n",
            "Reallyy, Greeaatt !?.;:)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Expand Contraction words"
      ],
      "metadata": {
        "id": "A2iSFuUyciLt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CONTRACTION_MAP = {\n",
        "\"ain't\": \"is not\",\n",
        "\"aren't\": \"are not\",\n",
        "\"can't\": \"cannot\",\n",
        "\"can't've\": \"cannot have\",\n",
        "\"'cause\": \"because\",\n",
        "\"could've\": \"could have\",\n",
        "\"couldn't\": \"could not\",\n",
        "\"couldn't've\": \"could not have\",\n",
        "\"didn't\": \"did not\",\n",
        "\"doesn't\": \"does not\",\n",
        "\"don't\": \"do not\",\n",
        "\"hadn't\": \"had not\",\n",
        "\"hadn't've\": \"had not have\",\n",
        "\"hasn't\": \"has not\",\n",
        "\"haven't\": \"have not\",\n",
        "\"he'd\": \"he would\",\n",
        "\"he'd've\": \"he would have\",\n",
        "\"he'll\": \"he will\",\n",
        "\"he'll've\": \"he he will have\",\n",
        "\"he's\": \"he is\",\n",
        "\"how'd\": \"how did\",\n",
        "\"how'd'y\": \"how do you\",\n",
        "\"how'll\": \"how will\",\n",
        "\"how's\": \"how is\",\n",
        "\"i'd\": \"i would\",\n",
        "\"i'd've\": \"i would have\",\n",
        "\"i'll\": \"i will\",\n",
        "\"i'll've\": \"i will have\",\n",
        "\"i'm\": \"i am\",\n",
        "\"i've\": \"i have\",\n",
        "\"isn't\": \"is not\",\n",
        "\"it'd\": \"it would\",\n",
        "\"it'd've\": \"it would have\",\n",
        "\"it'll\": \"it will\",\n",
        "\"it'll've\": \"it will have\",\n",
        "\"it's\": \"it is\",\n",
        "\"let's\": \"let us\",\n",
        "\"ma'am\": \"madam\",\n",
        "\"mayn't\": \"may not\",\n",
        "\"might've\": \"might have\",\n",
        "\"mightn't\": \"might not\",\n",
        "\"mightn't've\": \"might not have\",\n",
        "\"must've\": \"must have\",\n",
        "\"mustn't\": \"must not\",\n",
        "\"mustn't've\": \"must not have\",\n",
        "\"needn't\": \"need not\",\n",
        "\"needn't've\": \"need not have\",\n",
        "\"o'clock\": \"of the clock\",\n",
        "\"oughtn't\": \"ought not\",\n",
        "\"oughtn't've\": \"ought not have\",\n",
        "\"shan't\": \"shall not\",\n",
        "\"sha'n't\": \"shall not\",\n",
        "\"shan't've\": \"shall not have\",\n",
        "\"she'd\": \"she would\",\n",
        "\"she'd've\": \"she would have\",\n",
        "\"she'll\": \"she will\",\n",
        "\"she'll've\": \"she will have\",\n",
        "\"she's\": \"she is\",\n",
        "\"should've\": \"should have\",\n",
        "\"shouldn't\": \"should not\",\n",
        "\"shouldn't've\": \"should not have\",\n",
        "\"so've\": \"so have\",\n",
        "\"so's\": \"so as\",\n",
        "\"that'd\": \"that would\",\n",
        "\"that'd've\": \"that would have\",\n",
        "\"that's\": \"that is\",\n",
        "\"there'd\": \"there would\",\n",
        "\"there'd've\": \"there would have\",\n",
        "\"there's\": \"there is\",\n",
        "\"they'd\": \"they would\",\n",
        "\"they'd've\": \"they would have\",\n",
        "\"they'll\": \"they will\",\n",
        "\"they'll've\": \"they will have\",\n",
        "\"they're\": \"they are\",\n",
        "\"they've\": \"they have\",\n",
        "\"to've\": \"to have\",\n",
        "\"wasn't\": \"was not\",\n",
        "\"we'd\": \"we would\",\n",
        "\"we'd've\": \"we would have\",\n",
        "\"we'll\": \"we will\",\n",
        "\"we'll've\": \"we will have\",\n",
        "\"we're\": \"we are\",\n",
        "\"we've\": \"we have\",\n",
        "\"weren't\": \"were not\",\n",
        "\"what'll\": \"what will\",\n",
        "\"what'll've\": \"what will have\",\n",
        "\"what're\": \"what are\",\n",
        "\"what's\": \"what is\",\n",
        "\"what've\": \"what have\",\n",
        "\"when's\": \"when is\",\n",
        "\"when've\": \"when have\",\n",
        "\"where'd\": \"where did\",\n",
        "\"where's\": \"where is\",\n",
        "\"where've\": \"where have\",\n",
        "\"who'll\": \"who will\",\n",
        "\"who'll've\": \"who will have\",\n",
        "\"who's\": \"who is\",\n",
        "\"who've\": \"who have\",\n",
        "\"why's\": \"why is\",\n",
        "\"why've\": \"why have\",\n",
        "\"will've\": \"will have\",\n",
        "\"won't\": \"will not\",\n",
        "\"won't've\": \"will not have\",\n",
        "\"would've\": \"would have\",\n",
        "\"wouldn't\": \"would not\",\n",
        "\"wouldn't've\": \"would not have\",\n",
        "\"y'all\": \"you all\",\n",
        "\"y'all'd\": \"you all would\",\n",
        "\"y'all'd've\": \"you all would have\",\n",
        "\"y'all're\": \"you all are\",\n",
        "\"y'all've\": \"you all have\",\n",
        "\"you'd\": \"you would\",\n",
        "\"you'd've\": \"you would have\",\n",
        "\"you'll\": \"you will\",\n",
        "\"you'll've\": \"you will have\",\n",
        "\"you're\": \"you are\",\n",
        "\"you've\": \"you have\",\n",
        "}"
      ],
      "metadata": {
        "id": "ufL-Je2uJ0O2"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def expand_contraction_words(text, cont_map = CONTRACTION_MAP):\n",
        "  items = text.split(' ')\n",
        "  for item in items:\n",
        "    if item in CONTRACTION_MAP:\n",
        "      items = [sample.replace(item, CONTRACTION_MAP[item]) for sample in items]\n",
        "  items = ' '.join(str(e) for e in items)\n",
        "  return items\n",
        "check = \"ain't , aren't , can't , cause , can't've\"\n",
        "print(check)\n",
        "check1 = expand_contraction_words(check)\n",
        "print(check1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ga8jyIkKcmfH",
        "outputId": "9e4e6123-a112-458b-ecb0-1450a3e1c9c8"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ain't , aren't , can't , cause , can't've\n",
            "is not , are not , cannot , cause , cannot've\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Removing Special Characters"
      ],
      "metadata": {
        "id": "f-lSq-qhfUio"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_special_characters(text):\n",
        "  result = re.sub(r'[^a-zA-Z0-9:$-,%.?!]+', ' ', text)\n",
        "  return result\n",
        "check = ' Hello, K-a-j-a-l. Thi*s is $100.05 : the payment that you will recieve! (Is this okay?) '\n",
        "print(check)\n",
        "check1 = remove_special_characters(check)\n",
        "print(check1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9C_Lys2elgC",
        "outputId": "e339b373-07be-440f-c8ef-6be90a7588f9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Hello, K-a-j-a-l. Thi*s is $100.05 : the payment that you will recieve! (Is this okay?) \n",
            " Hello, K a j a l. Thi*s is $100.05 : the payment that you will recieve! (Is this okay?) \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Removing Stop Words"
      ],
      "metadata": {
        "id": "B2mY4EIxhkMW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stoplist = stopwords.words('english')\n",
        "stoplist = set(stoplist)\n",
        "def remove_stopwords(text):\n",
        "  text = repr(text)\n",
        "  result = [word for word in word_tokenize(text) if word.lower() not in stoplist ]\n",
        "  result = ' '.join(result)\n",
        "  result = result.replace(\"'\",'')\n",
        "  return result\n",
        "check = 'This is Kajal from the delhi who was came here to study.'\n",
        "print(check)\n",
        "check1 = remove_stopwords(check)\n",
        "print(check1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i0mWVtATgu0g",
        "outputId": "d739de84-a980-457b-eff4-75a3042e5c05"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is Kajal from the delhi who was came here to study.\n",
            "This Kajal delhi came study . \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Spelling Correction"
      ],
      "metadata": {
        "id": "cWPXEv6wkyxD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def spell_correction(text):\n",
        "  spell =Speller(lang='en')\n",
        "  result = spell(text)\n",
        "  return result\n",
        "check = 'This is Oberois from Dlhi who came hiree to stuydd'\n",
        "print(check)\n",
        "check1 = spell_correction(check)\n",
        "print(check1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MdetieJ0hyS3",
        "outputId": "ff0396cd-15c8-4e7a-f434-4a26a8500897"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is Oberois from Dlhi who came hiree to stuydd\n",
            "This is Oberoi from Delhi who came hired to study\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lemmatization"
      ],
      "metadata": {
        "id": "iJORDtkEmBvm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
        "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "def lemmatization(text):\n",
        "  result = [lemmatizer.lemmatize(w,'v') for w in tokenizer.tokenize(text)]\n",
        "  result = str(result)\n",
        "  result = result.replace('[', '').replace(']', '').replace(\"'\",'').replace(',','')\n",
        "  return result\n",
        "check = 'cats ext having root words only, no tense form, no plural are forms cats dogs'\n",
        "print(check)\n",
        "check1 = lemmatization(check)\n",
        "print(check1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZmJE8v-lVTC",
        "outputId": "735f1254-894a-492c-c00b-93f9a543332c"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cats ext having root words only, no tense form, no plural are forms cats dogs\n",
            "cat ext have root word only no tense form no plural be form cat dog\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aCg-8GBObr6-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}